[2025-03-26T12:41:52.109+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-26T12:41:52.230+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: historical_weather_etl.run_historical_script manual__2025-03-26T12:41:50.317726+00:00 [queued]>
[2025-03-26T12:41:52.241+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: historical_weather_etl.run_historical_script manual__2025-03-26T12:41:50.317726+00:00 [queued]>
[2025-03-26T12:41:52.245+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-03-26T12:41:52.259+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): run_historical_script> on 2025-03-26 12:41:50.317726+00:00
[2025-03-26T12:41:52.264+0000] {standard_task_runner.py:72} INFO - Started process 83669 to run task
[2025-03-26T12:41:52.267+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'historical_weather_etl', 'run_historical_script', 'manual__2025-03-26T12:41:50.317726+00:00', '--job-id', '89', '--raw', '--subdir', 'DAGS_FOLDER/historical_dag.py', '--cfg-path', '/tmp/tmp8d4ivhq_']
[2025-03-26T12:41:52.269+0000] {standard_task_runner.py:105} INFO - Job 89: Subtask run_historical_script
[2025-03-26T12:41:52.312+0000] {task_command.py:467} INFO - Running <TaskInstance: historical_weather_etl.run_historical_script manual__2025-03-26T12:41:50.317726+00:00 [running]> on host 3c3c1f66214b
[2025-03-26T12:41:52.386+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='historical_weather_etl' AIRFLOW_CTX_TASK_ID='run_historical_script' AIRFLOW_CTX_EXECUTION_DATE='2025-03-26T12:41:50.317726+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-26T12:41:50.317726+00:00'
[2025-03-26T12:41:52.388+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-03-26T12:41:52.389+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-03-26T12:41:52.390+0000] {logging_mixin.py:190} INFO - Current task name:run_historical_script state:running start_date:2025-03-26 12:41:52.232134+00:00
[2025-03-26T12:41:52.390+0000] {logging_mixin.py:190} INFO - Dag name:historical_weather_etl and current dag run status:running
[2025-03-26T12:41:52.391+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-26T12:41:52.392+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-03-26T12:41:52.393+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/dags/historical.py']
[2025-03-26T12:41:52.401+0000] {subprocess.py:99} INFO - Output:
[2025-03-26T12:41:54.880+0000] {subprocess.py:106} INFO - 25/03/26 12:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-26T12:41:55.050+0000] {subprocess.py:106} INFO - Setting default log level to "WARN".
[2025-03-26T12:41:55.052+0000] {subprocess.py:106} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-03-26T12:42:01.587+0000] {subprocess.py:106} INFO - [Stage 0:>                                                          (0 + 8) / 8]25/03/26 12:42:01 ERROR Executor: Exception in task 4.0 in stage 0.0 (TID 4)
[2025-03-26T12:42:01.589+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-26T12:42:01.590+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:01.592+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:01.593+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:01.599+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.603+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-26T12:42:01.605+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-26T12:42:01.606+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-26T12:42:01.608+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-26T12:42:01.609+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-26T12:42:01.611+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-26T12:42:01.612+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.613+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.615+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-26T12:42:01.617+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-26T12:42:01.618+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-26T12:42:01.620+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.621+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-26T12:42:01.622+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-26T12:42:01.623+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.624+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-26T12:42:01.626+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-26T12:42:01.628+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.629+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-26T12:42:01.630+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-26T12:42:01.632+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-26T12:42:01.635+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-26T12:42:01.636+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-26T12:42:01.637+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-26T12:42:01.640+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-26T12:42:01.642+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-26T12:42:01.643+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-26T12:42:01.645+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-26T12:42:01.646+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-26T12:42:01.648+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-26T12:42:01.651+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-26T12:42:01.652+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-26T12:42:01.653+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-26T12:42:01.654+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-26T12:42:01.655+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-26T12:42:01.657+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-26T12:42:01.658+0000] {subprocess.py:106} INFO - 25/03/26 12:42:01 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)
[2025-03-26T12:42:01.659+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-26T12:42:01.660+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:01.662+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:01.669+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:01.671+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.672+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-26T12:42:01.673+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-26T12:42:01.675+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-26T12:42:01.676+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-26T12:42:01.677+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-26T12:42:01.679+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-26T12:42:01.680+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.682+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.684+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-26T12:42:01.685+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-26T12:42:01.686+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-26T12:42:01.688+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.689+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-26T12:42:01.690+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-26T12:42:01.691+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.692+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-26T12:42:01.694+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-26T12:42:01.695+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.696+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-26T12:42:01.697+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-26T12:42:01.698+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-26T12:42:01.700+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-26T12:42:01.702+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-26T12:42:01.703+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-26T12:42:01.704+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-26T12:42:01.705+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-26T12:42:01.706+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-26T12:42:01.707+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-26T12:42:01.708+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-26T12:42:01.709+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-26T12:42:01.710+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-26T12:42:01.711+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-26T12:42:01.712+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-26T12:42:01.713+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-26T12:42:01.714+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-26T12:42:01.716+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-26T12:42:01.717+0000] {subprocess.py:106} INFO - 25/03/26 12:42:01 ERROR Executor: Exception in task 5.0 in stage 0.0 (TID 5)
[2025-03-26T12:42:01.719+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-26T12:42:01.720+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:01.721+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:01.722+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:01.723+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.724+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-26T12:42:01.725+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-26T12:42:01.726+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-26T12:42:01.727+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-26T12:42:01.728+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-26T12:42:01.729+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-26T12:42:01.730+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.732+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.733+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-26T12:42:01.734+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-26T12:42:01.735+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-26T12:42:01.736+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.737+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-26T12:42:01.738+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-26T12:42:01.739+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.740+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-26T12:42:01.741+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-26T12:42:01.742+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.743+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-26T12:42:01.744+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-26T12:42:01.745+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-26T12:42:01.746+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-26T12:42:01.747+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-26T12:42:01.749+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-26T12:42:01.751+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-26T12:42:01.752+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-26T12:42:01.753+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-26T12:42:01.754+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-26T12:42:01.755+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-26T12:42:01.756+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-26T12:42:01.757+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-26T12:42:01.759+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-26T12:42:01.760+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-26T12:42:01.761+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-26T12:42:01.762+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-26T12:42:01.763+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-26T12:42:01.765+0000] {subprocess.py:106} INFO - 25/03/26 12:42:01 ERROR Executor: Exception in task 7.0 in stage 0.0 (TID 7)
[2025-03-26T12:42:01.767+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-26T12:42:01.769+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:01.770+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:01.771+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:01.772+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.773+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-26T12:42:01.774+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-26T12:42:01.776+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-26T12:42:01.777+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-26T12:42:01.778+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-26T12:42:01.779+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-26T12:42:01.780+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.781+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.783+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-26T12:42:01.785+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-26T12:42:01.787+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-26T12:42:01.788+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.789+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-26T12:42:01.790+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-26T12:42:01.792+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.794+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-26T12:42:01.795+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-26T12:42:01.797+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.802+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-26T12:42:01.818+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-26T12:42:01.823+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-26T12:42:01.824+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-26T12:42:01.825+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-26T12:42:01.827+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-26T12:42:01.828+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-26T12:42:01.829+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-26T12:42:01.830+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-26T12:42:01.833+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-26T12:42:01.835+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-26T12:42:01.837+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-26T12:42:01.839+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-26T12:42:01.840+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-26T12:42:01.842+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-26T12:42:01.843+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-26T12:42:01.844+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-26T12:42:01.845+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-26T12:42:01.846+0000] {subprocess.py:106} INFO - 25/03/26 12:42:01 WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5) (3c3c1f66214b executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-26T12:42:01.848+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:01.851+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:01.853+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:01.855+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.857+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-26T12:42:01.858+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-26T12:42:01.859+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-26T12:42:01.860+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-26T12:42:01.862+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-26T12:42:01.863+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-26T12:42:01.865+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.867+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.868+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-26T12:42:01.869+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-26T12:42:01.870+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-26T12:42:01.871+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-26T12:42:01.872+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-26T12:42:01.874+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-26T12:42:01.875+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.876+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-26T12:42:01.877+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-26T12:42:01.878+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-26T12:42:01.879+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-26T12:42:01.880+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-26T12:42:01.881+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-26T12:42:01.882+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-26T12:42:01.885+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-26T12:42:01.887+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-26T12:42:01.889+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-26T12:42:01.890+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-26T12:42:01.891+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-26T12:42:01.892+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-26T12:42:01.894+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-26T12:42:01.895+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-26T12:42:01.896+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-26T12:42:01.897+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-26T12:42:01.899+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-26T12:42:01.901+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-26T12:42:01.902+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-26T12:42:01.903+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-26T12:42:01.904+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:01.905+0000] {subprocess.py:106} INFO - 25/03/26 12:42:01 ERROR TaskSetManager: Task 5 in stage 0.0 failed 1 times; aborting job
[2025-03-26T12:42:02.501+0000] {subprocess.py:106} INFO - [Stage 0:>                                                          (0 + 7) / 8]Error:
[2025-03-26T12:42:02.506+0000] {subprocess.py:106} INFO -   An exception was thrown from the Python worker. Please see the stack trace below.
[2025-03-26T12:42:02.510+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
[2025-03-26T12:42:02.514+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/historical.py", line 89, in map_to_description
[2025-03-26T12:42:02.520+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-26T12:42:02.524+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-26T12:42:02.528+0000] {subprocess.py:106} INFO - 
[2025-03-26T12:42:03.014+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-03-26T12:42:03.040+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-26T12:42:03.041+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=historical_weather_etl, task_id=run_historical_script, run_id=manual__2025-03-26T12:41:50.317726+00:00, execution_date=20250326T124150, start_date=20250326T124152, end_date=20250326T124203
[2025-03-26T12:42:03.068+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-03-26T12:42:03.070+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-03-26T12:42:03.071+0000] {logging_mixin.py:190} INFO - Dag name:historical_weather_etl queued_at:2025-03-26 12:41:50.330242+00:00
[2025-03-26T12:42:03.072+0000] {logging_mixin.py:190} INFO - Task hostname:3c3c1f66214b operator:BashOperator
[2025-03-26T12:42:03.119+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-26T12:42:03.134+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-26T12:42:03.137+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
