[2025-03-27T07:08:37.934+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-27T07:08:38.220+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: historical_weather_etl.run_historical_script scheduled__2025-03-26T00:00:00+00:00 [queued]>
[2025-03-27T07:08:38.241+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: historical_weather_etl.run_historical_script scheduled__2025-03-26T00:00:00+00:00 [queued]>
[2025-03-27T07:08:38.244+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-03-27T07:08:38.292+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): run_historical_script> on 2025-03-26 00:00:00+00:00
[2025-03-27T07:08:38.306+0000] {standard_task_runner.py:72} INFO - Started process 54 to run task
[2025-03-27T07:08:38.312+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'historical_weather_etl', 'run_historical_script', 'scheduled__2025-03-26T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/historical_dag.py', '--cfg-path', '/tmp/tmp_5nd1xp9']
[2025-03-27T07:08:38.318+0000] {standard_task_runner.py:105} INFO - Job 96: Subtask run_historical_script
[2025-03-27T07:08:38.431+0000] {task_command.py:467} INFO - Running <TaskInstance: historical_weather_etl.run_historical_script scheduled__2025-03-26T00:00:00+00:00 [running]> on host 3c3c1f66214b
[2025-03-27T07:08:38.714+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='historical_weather_etl' AIRFLOW_CTX_TASK_ID='run_historical_script' AIRFLOW_CTX_EXECUTION_DATE='2025-03-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-26T00:00:00+00:00'
[2025-03-27T07:08:38.717+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-03-27T07:08:38.719+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-03-27T07:08:38.721+0000] {logging_mixin.py:190} INFO - Current task name:run_historical_script state:running start_date:2025-03-27 07:08:38.223284+00:00
[2025-03-27T07:08:38.723+0000] {logging_mixin.py:190} INFO - Dag name:historical_weather_etl and current dag run status:running
[2025-03-27T07:08:38.725+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-27T07:08:38.728+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-03-27T07:08:38.730+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/dags/historical.py']
[2025-03-27T07:08:38.770+0000] {subprocess.py:99} INFO - Output:
[2025-03-27T07:08:48.330+0000] {subprocess.py:106} INFO - 25/03/27 07:08:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-27T07:08:48.920+0000] {subprocess.py:106} INFO - Setting default log level to "WARN".
[2025-03-27T07:08:48.939+0000] {subprocess.py:106} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-03-27T07:09:21.616+0000] {subprocess.py:106} INFO - [Stage 0:>                                                          (0 + 8) / 8]                                                                                25/03/27 07:09:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-03-27T07:09:38.997+0000] {subprocess.py:106} INFO - [Stage 4:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                25/03/27 07:09:38 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 58)
[2025-03-27T07:09:39.001+0000] {subprocess.py:106} INFO - java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.003+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.005+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.008+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.009+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.011+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.013+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.015+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.016+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.018+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.020+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.022+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.023+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.025+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.026+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.028+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.030+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.031+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.033+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.034+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.036+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.037+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.039+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.041+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.042+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.044+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.045+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.047+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.048+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.050+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.051+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.053+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.073+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.074+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.076+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.077+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.078+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.080+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.082+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.083+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.085+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.087+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.088+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.090+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.091+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.093+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.095+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.097+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.098+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.099+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.100+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.101+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.103+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.105+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.106+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.108+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.109+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.118+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.119+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.121+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.122+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.124+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.126+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.128+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.130+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.131+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job
[2025-03-27T07:09:39.133+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 14.0 in stage 12.0 (TID 72) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.151+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.169+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.171+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.172+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.174+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.175+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.177+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.178+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.184+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.186+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.188+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.191+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.193+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.195+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.197+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.200+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.206+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.208+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.209+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.211+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.213+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.214+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.216+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.217+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.219+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.221+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.222+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.224+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.230+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.231+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.232+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.234+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.235+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.236+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 11.0 in stage 12.0 (TID 69) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.238+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.239+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.240+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.242+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.243+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.244+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.246+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.247+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.248+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.249+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.251+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.253+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.254+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.255+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.256+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.257+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.259+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.260+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.265+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.266+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.267+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.269+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.270+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.272+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.273+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.275+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.276+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.278+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.280+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.282+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.284+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.285+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.287+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.289+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 13.0 in stage 12.0 (TID 71) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.290+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.292+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.293+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.295+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.297+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.298+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.304+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.416+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.440+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.442+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.444+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.447+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.450+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.452+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.454+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.456+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.457+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.459+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.461+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.462+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.464+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.465+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.467+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.468+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.470+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.472+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.473+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.475+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.476+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.478+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.480+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.481+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.483+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.485+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 10.0 in stage 12.0 (TID 68) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.487+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.488+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.490+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.491+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.493+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.494+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.496+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.498+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.499+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.501+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.502+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.504+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.506+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.507+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.508+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.510+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.511+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.513+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.514+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.515+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.516+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.518+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.520+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.521+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.523+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.524+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.525+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.527+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.528+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.529+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.531+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.532+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.533+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.534+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 16.0 in stage 12.0 (TID 74) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.536+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.537+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.538+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.539+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.540+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.542+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.543+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.544+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.545+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.547+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.548+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.549+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.551+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.552+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.554+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.555+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.556+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.558+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.559+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.561+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.562+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.564+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.565+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.566+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.568+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.569+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.570+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.571+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.573+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.574+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.576+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.577+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.578+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.580+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 15.0 in stage 12.0 (TID 73) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.582+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.583+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.585+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.586+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.588+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.589+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.591+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.592+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.594+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.596+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.598+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.599+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.601+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.602+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.604+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.605+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.606+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.607+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.609+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.610+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.611+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.612+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.614+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.615+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.616+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.617+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.618+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.619+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.620+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.622+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.623+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.624+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.625+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.626+0000] {subprocess.py:106} INFO - 25/03/27 07:09:39 WARN TaskSetManager: Lost task 12.0 in stage 12.0 (TID 70) (3c3c1f66214b executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.628+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.629+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.631+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.632+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.633+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.634+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.635+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.636+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.637+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.639+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.640+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.641+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.642+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.643+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.645+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.646+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.647+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.649+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.650+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.651+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.653+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.654+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.656+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.657+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.659+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.660+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.661+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.663+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.665+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.666+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.668+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.669+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.671+0000] {subprocess.py:106} INFO - Driver stacktrace:)
[2025-03-27T07:09:39.672+0000] {subprocess.py:106} INFO - +----------+----------------+-----------+--------+--------+---------+---------+-------------+----------------------------------------------+
[2025-03-27T07:09:39.674+0000] {subprocess.py:106} INFO - |city      |local_time      |temperature|wind_kph|humidity|feelslike|precip_mm|text         |icon                                          |
[2025-03-27T07:09:39.676+0000] {subprocess.py:106} INFO - +----------+----------------+-----------+--------+--------+---------+---------+-------------+----------------------------------------------+
[2025-03-27T07:09:39.679+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-01 04:00|7.6        |8.5     |68      |7.6      |0.1      |Light Drizzle|//cdn.weatherapi.com/weather/64x64/day/176.png|
[2025-03-27T07:09:39.681+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-01 09:00|8.0        |21.3    |73      |8.0      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.684+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-01 15:00|14.0       |32.0    |34      |14.0     |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.691+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-01 22:00|8.7        |19.9    |56      |8.7      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.696+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-02 04:00|6.5        |18.7    |63      |6.5      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.698+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-02 09:00|6.0        |22.1    |60      |6.0      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.707+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-02 15:00|11.8       |33.7    |31      |11.8     |0.0      |Partly Cloudy|//cdn.weatherapi.com/weather/64x64/day/116.png|
[2025-03-27T07:09:39.708+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-02 22:00|7.6        |21.7    |44      |7.6      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.710+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-03 04:00|6.6        |27.6    |46      |6.6      |0.0      |Cloudy       |//cdn.weatherapi.com/weather/64x64/day/119.png|
[2025-03-27T07:09:39.713+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-03 09:00|5.8        |21.3    |49      |5.8      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.715+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-03 15:00|13.4       |28.0    |27      |13.4     |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.716+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-03 22:00|7.6        |12.7    |48      |7.6      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.718+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-04 04:00|5.4        |17.4    |54      |5.4      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.719+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-04 09:00|5.0        |20.7    |60      |5.0      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.720+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-04 15:00|13.8       |12.6    |29      |13.8     |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.722+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-04 22:00|8.7        |6.0     |50      |8.7      |0.0      |Partly Cloudy|//cdn.weatherapi.com/weather/64x64/day/116.png|
[2025-03-27T07:09:39.723+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-05 04:00|5.4        |11.5    |72      |5.4      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.725+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-05 09:00|4.1        |15.0    |75      |4.1      |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.726+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-05 15:00|13.1       |5.5     |26      |13.1     |0.0      |Sunny        |//cdn.weatherapi.com/weather/64x64/day/113.png|
[2025-03-27T07:09:39.727+0000] {subprocess.py:106} INFO - |torrevieja|2021-01-05 22:00|7.0        |5.1     |56      |7.0      |0.0      |Cloudy       |//cdn.weatherapi.com/weather/64x64/day/119.png|
[2025-03-27T07:09:39.728+0000] {subprocess.py:106} INFO - +----------+----------------+-----------+--------+--------+---------+---------+-------------+----------------------------------------------+
[2025-03-27T07:09:39.730+0000] {subprocess.py:106} INFO - only showing top 20 rows
[2025-03-27T07:09:39.732+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.733+0000] {subprocess.py:106} INFO - Nombre de la carpeta: torrevieja_last_2_years
[2025-03-27T07:09:39.734+0000] {subprocess.py:106} INFO - Archivo Parquet generado: /tmp/tmp1yueqj4f/torrevieja_last_2_years/part-00000-451d075f-df9c-468f-9179-fc49118b0b79-c000.snappy.parquet
[2025-03-27T07:09:39.735+0000] {subprocess.py:106} INFO - Archivo subido correctamente a MinIO en: torrevieja_last_2_years/torrevieja_last_2_years.parquet
[2025-03-27T07:09:39.736+0000] {subprocess.py:106} INFO - Error: An error occurred while calling o198.save.
[2025-03-27T07:09:39.738+0000] {subprocess.py:106} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 58) (3c3c1f66214b executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.740+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.741+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.743+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.744+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.746+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.747+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.748+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.750+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.751+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.752+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.754+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.755+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.756+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.758+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.760+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.764+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.766+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.768+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.769+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.771+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.772+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.774+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.775+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.777+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.780+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.783+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.784+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.786+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.787+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.789+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.790+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:39.792+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:39.793+0000] {subprocess.py:106} INFO - Driver stacktrace:
[2025-03-27T07:09:39.794+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-03-27T07:09:39.797+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-03-27T07:09:39.799+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-03-27T07:09:39.801+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-03-27T07:09:39.802+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-03-27T07:09:39.804+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-03-27T07:09:39.805+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-03-27T07:09:39.807+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-03-27T07:09:39.808+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-03-27T07:09:39.810+0000] {subprocess.py:106} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-03-27T07:09:39.811+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-03-27T07:09:39.813+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-03-27T07:09:39.815+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-03-27T07:09:39.816+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-03-27T07:09:39.818+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-03-27T07:09:39.820+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-03-27T07:09:39.821+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2025-03-27T07:09:39.823+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
[2025-03-27T07:09:39.824+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
[2025-03-27T07:09:39.825+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
[2025-03-27T07:09:39.826+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
[2025-03-27T07:09:39.827+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-03-27T07:09:39.829+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2025-03-27T07:09:39.830+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
[2025-03-27T07:09:39.831+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
[2025-03-27T07:09:39.832+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)
[2025-03-27T07:09:39.834+0000] {subprocess.py:106} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-03-27T07:09:39.835+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)
[2025-03-27T07:09:39.836+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-03-27T07:09:39.837+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-03-27T07:09:39.838+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-03-27T07:09:39.839+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-03-27T07:09:39.841+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-03-27T07:09:39.842+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)
[2025-03-27T07:09:39.843+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)
[2025-03-27T07:09:39.844+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)
[2025-03-27T07:09:39.845+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
[2025-03-27T07:09:39.846+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
[2025-03-27T07:09:39.847+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2025-03-27T07:09:39.849+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2025-03-27T07:09:39.850+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2025-03-27T07:09:39.851+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2025-03-27T07:09:39.852+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-03-27T07:09:39.853+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-03-27T07:09:39.854+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-03-27T07:09:39.855+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-03-27T07:09:39.856+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-03-27T07:09:39.857+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2025-03-27T07:09:39.858+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-03-27T07:09:39.859+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2025-03-27T07:09:39.860+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2025-03-27T07:09:39.861+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2025-03-27T07:09:39.862+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2025-03-27T07:09:39.864+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-03-27T07:09:39.865+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-03-27T07:09:39.866+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-03-27T07:09:39.867+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-03-27T07:09:39.868+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2025-03-27T07:09:39.869+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2025-03-27T07:09:39.870+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2025-03-27T07:09:39.871+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2025-03-27T07:09:39.872+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2025-03-27T07:09:39.873+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
[2025-03-27T07:09:39.874+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
[2025-03-27T07:09:39.875+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
[2025-03-27T07:09:39.876+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
[2025-03-27T07:09:39.877+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-03-27T07:09:39.878+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-03-27T07:09:39.879+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-03-27T07:09:39.880+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-03-27T07:09:39.881+0000] {subprocess.py:106} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-03-27T07:09:39.883+0000] {subprocess.py:106} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-03-27T07:09:39.884+0000] {subprocess.py:106} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-03-27T07:09:39.885+0000] {subprocess.py:106} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-03-27T07:09:39.886+0000] {subprocess.py:106} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-03-27T07:09:39.887+0000] {subprocess.py:106} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-03-27T07:09:39.889+0000] {subprocess.py:106} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-03-27T07:09:39.890+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-27T07:09:39.891+0000] {subprocess.py:106} INFO - Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO daily_weather ("city","local_time","temperature","wind_kph","humidity","feelslike","precip_mm","text","icon") VALUES ('torrevieja','2021-01-01 04:00',7.6,8.5,68,7.6,0.1,'Light Drizzle','//cdn.weatherapi.com/weather/64x64/day/176.png') was aborted: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.892+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.  Call getNextException to see other errors in the batch.
[2025-03-27T07:09:39.893+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:171)
[2025-03-27T07:09:39.894+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2298)
[2025-03-27T07:09:39.896+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1416)
[2025-03-27T07:09:39.897+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1441)
[2025-03-27T07:09:39.898+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:506)
[2025-03-27T07:09:39.899+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
[2025-03-27T07:09:39.900+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
[2025-03-27T07:09:39.901+0000] {subprocess.py:106} INFO - 	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
[2025-03-27T07:09:39.903+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
[2025-03-27T07:09:39.904+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
[2025-03-27T07:09:39.905+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
[2025-03-27T07:09:39.907+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
[2025-03-27T07:09:39.908+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
[2025-03-27T07:09:39.909+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2025-03-27T07:09:39.911+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-27T07:09:39.912+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-27T07:09:39.914+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-27T07:09:39.915+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-27T07:09:39.916+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-27T07:09:39.917+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-27T07:09:39.918+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-27T07:09:39.919+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-27T07:09:39.920+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-27T07:09:39.921+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-27T07:09:39.922+0000] {subprocess.py:106} INFO - 	... 1 more
[2025-03-27T07:09:39.924+0000] {subprocess.py:106} INFO - Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "unique_city_local_time"
[2025-03-27T07:09:39.925+0000] {subprocess.py:106} INFO -   Detail: Key (city, local_time)=(torrevieja, 2021-01-01 04:00) already exists.
[2025-03-27T07:09:39.926+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)
[2025-03-27T07:09:39.927+0000] {subprocess.py:106} INFO - 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)
[2025-03-27T07:09:39.928+0000] {subprocess.py:106} INFO - 	... 23 more
[2025-03-27T07:09:40.046+0000] {subprocess.py:106} INFO - 
[2025-03-27T07:09:40.650+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-03-27T07:09:40.703+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-27T07:09:40.705+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=historical_weather_etl, task_id=run_historical_script, run_id=scheduled__2025-03-26T00:00:00+00:00, execution_date=20250326T000000, start_date=20250327T070838, end_date=20250327T070940
[2025-03-27T07:09:40.748+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-03-27T07:09:40.750+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-03-27T07:09:40.752+0000] {logging_mixin.py:190} INFO - Dag name:historical_weather_etl queued_at:2025-03-27 07:08:20.880079+00:00
[2025-03-27T07:09:40.754+0000] {logging_mixin.py:190} INFO - Task hostname:3c3c1f66214b operator:BashOperator
[2025-03-27T07:09:40.780+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-27T07:09:40.797+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
