[2025-03-31T07:08:52.225+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-31T07:08:52.414+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T07:08:35.698092+00:00 [queued]>
[2025-03-31T07:08:52.434+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T07:08:35.698092+00:00 [queued]>
[2025-03-31T07:08:52.436+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-03-31T07:08:52.455+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): run_minio_script> on 2025-03-31 07:08:35.698092+00:00
[2025-03-31T07:08:52.464+0000] {standard_task_runner.py:72} INFO - Started process 4922 to run task
[2025-03-31T07:08:52.470+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'daily_weather_etl', 'run_minio_script', 'manual__2025-03-31T07:08:35.698092+00:00', '--job-id', '188', '--raw', '--subdir', 'DAGS_FOLDER/weather_dag.py', '--cfg-path', '/tmp/tmp6ewnlulh']
[2025-03-31T07:08:52.473+0000] {standard_task_runner.py:105} INFO - Job 188: Subtask run_minio_script
[2025-03-31T07:08:52.546+0000] {task_command.py:467} INFO - Running <TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T07:08:35.698092+00:00 [running]> on host 3c3c1f66214b
[2025-03-31T07:08:52.685+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='daily_weather_etl' AIRFLOW_CTX_TASK_ID='run_minio_script' AIRFLOW_CTX_EXECUTION_DATE='2025-03-31T07:08:35.698092+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-31T07:08:35.698092+00:00'
[2025-03-31T07:08:52.687+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-03-31T07:08:52.689+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-03-31T07:08:52.690+0000] {logging_mixin.py:190} INFO - Current task name:run_minio_script state:running start_date:2025-03-31 07:08:52.415959+00:00
[2025-03-31T07:08:52.692+0000] {logging_mixin.py:190} INFO - Dag name:daily_weather_etl and current dag run status:running
[2025-03-31T07:08:52.694+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-31T07:08:52.698+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-03-31T07:08:52.700+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/dags/minio.py']
[2025-03-31T07:08:52.718+0000] {subprocess.py:99} INFO - Output:
[2025-03-31T07:08:56.645+0000] {subprocess.py:106} INFO - Setting default log level to "WARN".
[2025-03-31T07:08:56.647+0000] {subprocess.py:106} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-03-31T07:08:56.936+0000] {subprocess.py:106} INFO - 25/03/31 07:08:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-31T07:09:07.546+0000] {subprocess.py:106} INFO - [Stage 0:>                                                          (0 + 8) / 8]25/03/31 07:09:07 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)
[2025-03-31T07:09:07.549+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T07:09:07.550+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T07:09:07.552+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T07:09:07.554+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T07:09:07.555+0000] {subprocess.py:106} INFO - 
[2025-03-31T07:09:07.557+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T07:09:07.558+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T07:09:07.564+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T07:09:07.566+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T07:09:07.568+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T07:09:07.569+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T07:09:07.571+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.573+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.574+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T07:09:07.576+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T07:09:07.577+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T07:09:07.579+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.581+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T07:09:07.583+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T07:09:07.584+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.586+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T07:09:07.588+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T07:09:07.590+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.591+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T07:09:07.597+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T07:09:07.599+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T07:09:07.600+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T07:09:07.602+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T07:09:07.603+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T07:09:07.605+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T07:09:07.608+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T07:09:07.611+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T07:09:07.613+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T07:09:07.614+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T07:09:07.616+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T07:09:07.617+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T07:09:07.619+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T07:09:07.620+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T07:09:07.622+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T07:09:07.624+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T07:09:07.626+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T07:09:07.628+0000] {subprocess.py:106} INFO - 25/03/31 07:09:07 ERROR Executor: Exception in task 7.0 in stage 0.0 (TID 7)
[2025-03-31T07:09:07.629+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T07:09:07.631+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T07:09:07.633+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T07:09:07.635+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T07:09:07.636+0000] {subprocess.py:106} INFO - 
[2025-03-31T07:09:07.637+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T07:09:07.639+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T07:09:07.640+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T07:09:07.643+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T07:09:07.646+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T07:09:07.654+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T07:09:07.656+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.658+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.662+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T07:09:07.664+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T07:09:07.666+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T07:09:07.667+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.669+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T07:09:07.672+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T07:09:07.674+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.675+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T07:09:07.677+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T07:09:07.678+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.680+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T07:09:07.682+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T07:09:07.684+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T07:09:07.685+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T07:09:07.687+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T07:09:07.689+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T07:09:07.691+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T07:09:07.692+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T07:09:07.694+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T07:09:07.696+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T07:09:07.697+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T07:09:07.699+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T07:09:07.701+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T07:09:07.702+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T07:09:07.704+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T07:09:07.706+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T07:09:07.707+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T07:09:07.709+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T07:09:07.711+0000] {subprocess.py:106} INFO - 25/03/31 07:09:07 WARN TaskSetManager: Lost task 6.0 in stage 0.0 (TID 6) (3c3c1f66214b executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T07:09:07.712+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T07:09:07.715+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T07:09:07.717+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T07:09:07.720+0000] {subprocess.py:106} INFO - 
[2025-03-31T07:09:07.722+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T07:09:07.724+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T07:09:07.726+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T07:09:07.727+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T07:09:07.729+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T07:09:07.731+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T07:09:07.733+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.735+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.737+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T07:09:07.738+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T07:09:07.740+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T07:09:07.741+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T07:09:07.743+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T07:09:07.745+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T07:09:07.746+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.748+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T07:09:07.750+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T07:09:07.752+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T07:09:07.753+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T07:09:07.755+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T07:09:07.757+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T07:09:07.758+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T07:09:07.760+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T07:09:07.761+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T07:09:07.763+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T07:09:07.764+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T07:09:07.766+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T07:09:07.768+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T07:09:07.769+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T07:09:07.771+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T07:09:07.772+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T07:09:07.774+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T07:09:07.775+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T07:09:07.777+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T07:09:07.779+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T07:09:07.780+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T07:09:07.782+0000] {subprocess.py:106} INFO - 
[2025-03-31T07:09:07.783+0000] {subprocess.py:106} INFO - 25/03/31 07:09:07 ERROR TaskSetManager: Task 6 in stage 0.0 failed 1 times; aborting job
[2025-03-31T07:09:08.647+0000] {subprocess.py:106} INFO - Error:
[2025-03-31T07:09:08.659+0000] {subprocess.py:106} INFO -   An exception was thrown from the Python worker. Please see the stack trace below.
[2025-03-31T07:09:08.665+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
[2025-03-31T07:09:08.678+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T07:09:08.691+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T07:09:08.699+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T07:09:08.706+0000] {subprocess.py:106} INFO - 
[2025-03-31T07:09:09.308+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-03-31T07:09:09.347+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-31T07:09:09.349+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=daily_weather_etl, task_id=run_minio_script, run_id=manual__2025-03-31T07:08:35.698092+00:00, execution_date=20250331T070835, start_date=20250331T070852, end_date=20250331T070909
[2025-03-31T07:09:09.386+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-03-31T07:09:09.388+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-03-31T07:09:09.389+0000] {logging_mixin.py:190} INFO - Dag name:daily_weather_etl queued_at:2025-03-31 07:08:35.782847+00:00
[2025-03-31T07:09:09.391+0000] {logging_mixin.py:190} INFO - Task hostname:3c3c1f66214b operator:BashOperator
[2025-03-31T07:09:09.443+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-31T07:09:09.475+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-31T07:09:09.479+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
