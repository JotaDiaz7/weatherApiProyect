[2025-03-31T06:49:07.283+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-31T06:49:07.461+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T06:48:46.622063+00:00 [queued]>
[2025-03-31T06:49:07.478+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T06:48:46.622063+00:00 [queued]>
[2025-03-31T06:49:07.480+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-03-31T06:49:07.501+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): run_minio_script> on 2025-03-31 06:48:46.622063+00:00
[2025-03-31T06:49:07.510+0000] {standard_task_runner.py:72} INFO - Started process 3269 to run task
[2025-03-31T06:49:07.516+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'daily_weather_etl', 'run_minio_script', 'manual__2025-03-31T06:48:46.622063+00:00', '--job-id', '180', '--raw', '--subdir', 'DAGS_FOLDER/weather_dag.py', '--cfg-path', '/tmp/tmpl2y7omg7']
[2025-03-31T06:49:07.519+0000] {standard_task_runner.py:105} INFO - Job 180: Subtask run_minio_script
[2025-03-31T06:49:07.588+0000] {task_command.py:467} INFO - Running <TaskInstance: daily_weather_etl.run_minio_script manual__2025-03-31T06:48:46.622063+00:00 [running]> on host 3c3c1f66214b
[2025-03-31T06:49:07.740+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='daily_weather_etl' AIRFLOW_CTX_TASK_ID='run_minio_script' AIRFLOW_CTX_EXECUTION_DATE='2025-03-31T06:48:46.622063+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-31T06:48:46.622063+00:00'
[2025-03-31T06:49:07.745+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-03-31T06:49:07.747+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-03-31T06:49:07.749+0000] {logging_mixin.py:190} INFO - Current task name:run_minio_script state:running start_date:2025-03-31 06:49:07.463740+00:00
[2025-03-31T06:49:07.750+0000] {logging_mixin.py:190} INFO - Dag name:daily_weather_etl and current dag run status:running
[2025-03-31T06:49:07.752+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-31T06:49:07.755+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-03-31T06:49:07.757+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/dags/minio.py']
[2025-03-31T06:49:07.778+0000] {subprocess.py:99} INFO - Output:
[2025-03-31T06:49:12.182+0000] {subprocess.py:106} INFO - Setting default log level to "WARN".
[2025-03-31T06:49:12.184+0000] {subprocess.py:106} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-03-31T06:49:12.405+0000] {subprocess.py:106} INFO - 25/03/31 06:49:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-31T06:49:24.782+0000] {subprocess.py:106} INFO - [Stage 0:>                                                          (0 + 8) / 8]25/03/31 06:49:24 ERROR Executor: Exception in task 7.0 in stage 0.0 (TID 7)
[2025-03-31T06:49:24.788+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T06:49:24.791+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T06:49:24.794+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T06:49:24.797+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T06:49:24.799+0000] {subprocess.py:106} INFO - 
[2025-03-31T06:49:24.801+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T06:49:24.803+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T06:49:24.813+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T06:49:24.815+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T06:49:24.819+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T06:49:24.821+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T06:49:24.823+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.824+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.826+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T06:49:24.828+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T06:49:24.830+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T06:49:24.832+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.836+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T06:49:24.839+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T06:49:24.844+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T06:49:24.846+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T06:49:24.847+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T06:49:24.848+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T06:49:24.855+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T06:49:24.858+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T06:49:24.861+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T06:49:24.863+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T06:49:24.864+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T06:49:24.866+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T06:49:24.869+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T06:49:24.873+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T06:49:24.876+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T06:49:24.879+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T06:49:24.880+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T06:49:24.882+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T06:49:24.884+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T06:49:24.887+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T06:49:24.890+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T06:49:24.891+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T06:49:24.893+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T06:49:24.902+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T06:49:24.904+0000] {subprocess.py:106} INFO - 25/03/31 06:49:24 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)
[2025-03-31T06:49:24.906+0000] {subprocess.py:106} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T06:49:24.907+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T06:49:24.909+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T06:49:24.910+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T06:49:24.912+0000] {subprocess.py:106} INFO - 
[2025-03-31T06:49:24.913+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T06:49:24.915+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T06:49:24.922+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T06:49:24.926+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T06:49:24.927+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T06:49:24.929+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T06:49:24.930+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.932+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.938+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T06:49:24.940+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T06:49:24.942+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T06:49:24.944+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:24.948+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T06:49:24.953+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T06:49:24.956+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T06:49:24.958+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T06:49:24.964+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T06:49:24.967+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T06:49:24.969+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T06:49:24.971+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T06:49:24.973+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T06:49:24.975+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T06:49:24.986+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T06:49:24.991+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T06:49:24.993+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T06:49:25.002+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T06:49:25.006+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T06:49:25.010+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T06:49:25.015+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T06:49:25.024+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T06:49:25.026+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T06:49:25.027+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T06:49:25.029+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T06:49:25.031+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T06:49:25.035+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T06:49:25.039+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T06:49:25.042+0000] {subprocess.py:106} INFO - 25/03/31 06:49:24 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7) (3c3c1f66214b executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-03-31T06:49:25.043+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T06:49:25.045+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T06:49:25.047+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T06:49:25.048+0000] {subprocess.py:106} INFO - 
[2025-03-31T06:49:25.054+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-03-31T06:49:25.056+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)
[2025-03-31T06:49:25.059+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)
[2025-03-31T06:49:25.061+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-03-31T06:49:25.062+0000] {subprocess.py:106} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-03-31T06:49:25.065+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2025-03-31T06:49:25.068+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:25.070+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:25.073+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-03-31T06:49:25.077+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-03-31T06:49:25.079+0000] {subprocess.py:106} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-03-31T06:49:25.081+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-03-31T06:49:25.085+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2025-03-31T06:49:25.087+0000] {subprocess.py:106} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2025-03-31T06:49:25.089+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2025-03-31T06:49:25.091+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)
[2025-03-31T06:49:25.092+0000] {subprocess.py:106} INFO - 	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)
[2025-03-31T06:49:25.094+0000] {subprocess.py:106} INFO - 	at scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)
[2025-03-31T06:49:25.097+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)
[2025-03-31T06:49:25.102+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)
[2025-03-31T06:49:25.104+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)
[2025-03-31T06:49:25.106+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)
[2025-03-31T06:49:25.113+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-03-31T06:49:25.116+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-03-31T06:49:25.122+0000] {subprocess.py:106} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-03-31T06:49:25.124+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-03-31T06:49:25.140+0000] {subprocess.py:106} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-03-31T06:49:25.142+0000] {subprocess.py:106} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-03-31T06:49:25.144+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-03-31T06:49:25.153+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-03-31T06:49:25.156+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-03-31T06:49:25.158+0000] {subprocess.py:106} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-03-31T06:49:25.159+0000] {subprocess.py:106} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-03-31T06:49:25.174+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-03-31T06:49:25.175+0000] {subprocess.py:106} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-03-31T06:49:25.177+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-03-31T06:49:25.191+0000] {subprocess.py:106} INFO - 
[2025-03-31T06:49:25.193+0000] {subprocess.py:106} INFO - 25/03/31 06:49:24 ERROR TaskSetManager: Task 7 in stage 0.0 failed 1 times; aborting job
[2025-03-31T06:49:25.963+0000] {subprocess.py:106} INFO - Error:
[2025-03-31T06:49:25.966+0000] {subprocess.py:106} INFO -   An exception was thrown from the Python worker. Please see the stack trace below.
[2025-03-31T06:49:25.970+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
[2025-03-31T06:49:25.972+0000] {subprocess.py:106} INFO -   File "/opt/***/dags/utils.py", line 179, in map_to_description
[2025-03-31T06:49:25.974+0000] {subprocess.py:106} INFO -     return icon_mapping[weathercode]['description']
[2025-03-31T06:49:25.975+0000] {subprocess.py:106} INFO - KeyError: None
[2025-03-31T06:49:25.977+0000] {subprocess.py:106} INFO - 
[2025-03-31T06:49:26.593+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-03-31T06:49:26.629+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-31T06:49:26.631+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=daily_weather_etl, task_id=run_minio_script, run_id=manual__2025-03-31T06:48:46.622063+00:00, execution_date=20250331T064846, start_date=20250331T064907, end_date=20250331T064926
[2025-03-31T06:49:26.668+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-03-31T06:49:26.670+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-03-31T06:49:26.671+0000] {logging_mixin.py:190} INFO - Dag name:daily_weather_etl queued_at:2025-03-31 06:48:46.636833+00:00
[2025-03-31T06:49:26.672+0000] {logging_mixin.py:190} INFO - Task hostname:3c3c1f66214b operator:BashOperator
[2025-03-31T06:49:26.698+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-31T06:49:26.723+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-31T06:49:26.726+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
